{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import kagglehub\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.10)\n",
      "Path to dataset files: C:\\Users\\keep_\\.cache\\kagglehub\\datasets\\raj713335\\twittesentimentanalysis\\versions\\1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target          ID                          Date     Query  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              User                                               Text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"raj713335/twittesentimentanalysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "tweets = pd.read_csv(f\"{path}/tweets.csv\", encoding = \"ISO-8859-1\", names= [\"Target\", \"ID\", \"Date\", \"Query\", \"User\", \"Text\"], header=None)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total samples: 1600000\n",
      "\n",
      "Initializing model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data...\n",
      "Number of batches: 2000\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|‚ñè         | 48/2000 [00:10<07:03,  4.61it/s, loss=0.6956, speed=4.6 it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 165\u001b[0m\n\u001b[0;32m    162\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 165\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 156\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Cell \u001b[1;32mIn[14], line 100\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, config, device)\u001b[0m\n\u001b[0;32m     97\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len=32):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def prepare_data(df, tokenizer, config):\n",
    "    # Convert targets from 0,4 to 0,1\n",
    "    df['Target'] = df['Target'].map({0: 0, 4: 1})\n",
    "    \n",
    "    # Sample balanced data\n",
    "    df_neg = df[df['Target'] == 0]\n",
    "    df_pos = df[df['Target'] == 1]\n",
    "    \n",
    "    n_samples = min(len(df_neg), len(df_pos)) // config['portion']\n",
    "    \n",
    "    df_neg = df_neg.sample(n=n_samples, random_state=42)\n",
    "    df_pos = df_pos.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    df_sampled = pd.concat([df_neg, df_pos]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TweetDataset(\n",
    "        texts=df_sampled['Text'].values,\n",
    "        targets=df_sampled['Target'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=config['max_len']\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0  # No parallel loading for debugging\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def train_model(model, dataloader, config, device):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f'Training')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move data to GPU\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += outputs.loss.item()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'speed': f'{batch_idx / (time.time() - start_time):.1f} it/s'\n",
    "            })\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'max_len': 32,          # Reduced for memory efficiency\n",
    "        'batch_size': 8,        # Small batch size for GTX 1650 Ti\n",
    "        'portion': 100,         # Use 1/100th of data\n",
    "        'epochs': 2,            # Start with 1 epoch\n",
    "        'learning_rate': 2e-5\n",
    "    }\n",
    "    \n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(\n",
    "        f\"{path}/tweets.csv\",\n",
    "        encoding=\"ISO-8859-1\",\n",
    "        names=[\"Target\", \"ID\", \"Date\", \"Query\", \"User\", \"Text\"],\n",
    "        header=None\n",
    "    )\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    print(\"\\nInitializing model and tokenizer...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    dataloader = prepare_data(df, tokenizer, CONFIG)\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_model(model, dataloader, CONFIG, device)\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 268\u001b[0m\n\u001b[0;32m    265\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 227\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Clear memory\u001b[39;00m\n\u001b[0;32m    226\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m--> 227\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len=48):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def prepare_data(df, tokenizer, config):\n",
    "    # Convert targets from 0,4 to 0,1\n",
    "    df['Target'] = df['Target'].map({0: 0, 4: 1})\n",
    "    \n",
    "    # Sample balanced data\n",
    "    df_neg = df[df['Target'] == 0]\n",
    "    df_pos = df[df['Target'] == 1]\n",
    "    \n",
    "    n_samples = min(len(df_neg), len(df_pos)) // config['portion']\n",
    "    \n",
    "    df_neg = df_neg.sample(n=n_samples, random_state=42)\n",
    "    df_pos = df_pos.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    df_sampled = pd.concat([df_neg, df_pos]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_df, val_df = train_test_split(df_sampled, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TweetDataset(\n",
    "        texts=train_df['Text'].values,\n",
    "        targets=train_df['Target'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=config['max_len']\n",
    "    )\n",
    "    \n",
    "    val_dataset = TweetDataset(\n",
    "        texts=val_df['Text'].values,\n",
    "        targets=val_df['Target'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=config['max_len']\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, accuracy, config, checkpoint_dir):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}_{timestamp}.pt')\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'config': config\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, device, checkpoint_dir):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Calculate total steps and warmup\n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    warmup_steps = total_steps // 10\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc='Training')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move data to GPU\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += outputs.loss.item()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'speed': f'{batch_idx / (time.time() - start_time):.1f} it/s',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        print(\"\\nRunning validation...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=targets\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_accuracy = 100 * correct / total\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "        print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {epoch_val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Save checkpoint if best model\n",
    "        if epoch_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_accuracy\n",
    "            best_val_loss = epoch_val_loss\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch + 1,\n",
    "                best_val_loss, best_val_accuracy,\n",
    "                config, checkpoint_dir\n",
    "            )\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'max_len': 48,\n",
    "        'batch_size': 16,\n",
    "        'portion': 10,\n",
    "        'epochs': 3,\n",
    "        'learning_rate': 2e-5\n",
    "    }\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = 'model_checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(\n",
    "        f\"{path}/tweets.csv\",\n",
    "        encoding=\"ISO-8859-1\",\n",
    "        names=[\"Target\", \"ID\", \"Date\", \"Query\", \"User\", \"Text\"],\n",
    "        header=None\n",
    "    )\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    print(\"\\nInitializing model and tokenizer...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    train_loader, val_loader = prepare_data(df, tokenizer, CONFIG)\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_model(model, train_loader, val_loader, CONFIG, device, checkpoint_dir)\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total samples: 1600000\n",
      "\n",
      "Initializing model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data...\n",
      "Number of training batches: 90000\n",
      "Number of validation batches: 5000\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Model Size: 255.42 MB\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90000/90000 [11:51:39<00:00,  2.11it/s]  \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [19:58<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Results:\n",
      "Training Loss: 0.3436\n",
      "Validation Accuracy: 86.50%\n",
      "Checkpoint saved: model_checkpoints\\checkpoint_epoch_1_20250308_085125.pt\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90000/90000 [9:40:56<00:00,  2.58it/s]   \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [18:16<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Results:\n",
      "Training Loss: 0.2762\n",
      "Validation Accuracy: 86.97%\n",
      "Checkpoint saved: model_checkpoints\\checkpoint_epoch_2_20250308_185045.pt\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90000/90000 [10:20:10<00:00,  2.42it/s]   \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [17:14<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Results:\n",
      "Training Loss: 0.2192\n",
      "Validation Accuracy: 86.90%\n",
      "\n",
      "Final Training Metrics:\n",
      "Average Training Time per Sample: 4.14 ms\n",
      "Average Evaluation Time per Sample: 0.55 ms\n",
      "First Epoch Accuracy: 86.50%\n",
      "Final Accuracy: 86.90%\n",
      "Average Epoch Time: 39365.72 seconds\n",
      "Total Training Time: 118113.66 seconds\n",
      "Model Size: 255.42 MB\n",
      "\n",
      "Average Resource Usage:\n",
      "CPU Usage: 27.6%\n",
      "RAM Usage: 69.5%\n",
      "GPU Utilization: 91.2%\n",
      "GPU Memory Usage: 46.6%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import psutil\n",
    "import numpy as np\n",
    "import GPUtil\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len=48):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.epoch_times = []\n",
    "        self.training_times = []\n",
    "        self.evaluation_times = []\n",
    "        self.training_samples = 0\n",
    "        self.evaluation_samples = 0\n",
    "        self.accuracies = []\n",
    "        self.resource_usage = []\n",
    "        \n",
    "    def start_training(self):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def start_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "    def end_epoch(self):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "    def add_batch_training_time(self, batch_time, batch_size):\n",
    "        self.training_times.append(batch_time)\n",
    "        self.training_samples += batch_size\n",
    "        \n",
    "    def add_batch_evaluation_time(self, batch_time, batch_size):\n",
    "        self.evaluation_times.append(batch_time)\n",
    "        self.evaluation_samples += batch_size\n",
    "        \n",
    "    def add_accuracy(self, accuracy):\n",
    "        self.accuracies.append(accuracy)\n",
    "        \n",
    "    def add_resource_usage(self, cpu_percent, ram_percent, gpu_percent, gpu_memory):\n",
    "        self.resource_usage.append({\n",
    "            'cpu': cpu_percent,\n",
    "            'ram': ram_percent,\n",
    "            'gpu_util': gpu_percent,\n",
    "            'gpu_memory': gpu_memory\n",
    "        })\n",
    "        \n",
    "    def get_metrics(self):\n",
    "        total_training_time = time.time() - self.start_time\n",
    "        avg_training_time = sum(self.training_times) / self.training_samples\n",
    "        avg_evaluation_time = sum(self.evaluation_times) / self.evaluation_samples\n",
    "        avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)\n",
    "        \n",
    "        # Calculate average resource usage\n",
    "        avg_resources = {\n",
    "            'cpu': np.mean([r['cpu'] for r in self.resource_usage]),\n",
    "            'ram': np.mean([r['ram'] for r in self.resource_usage]),\n",
    "            'gpu_util': np.mean([r['gpu_util'] for r in self.resource_usage]),\n",
    "            'gpu_memory': np.mean([r['gpu_memory'] for r in self.resource_usage])\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'avg_training_time_per_sample': avg_training_time,\n",
    "            'avg_evaluation_time_per_sample': avg_evaluation_time,\n",
    "            'first_epoch_accuracy': self.accuracies[0],\n",
    "            'final_accuracy': self.accuracies[-1],\n",
    "            'avg_epoch_time': avg_epoch_time,\n",
    "            'total_training_time': total_training_time,\n",
    "            'avg_resources': avg_resources\n",
    "        }\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "def get_system_resources():\n",
    "    # CPU\n",
    "    cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "    \n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    ram_percent = ram.percent\n",
    "    \n",
    "    # GPU\n",
    "    gpu = GPUtil.getGPUs()[0]\n",
    "    gpu_util = gpu.load * 100\n",
    "    gpu_memory = gpu.memoryUtil * 100\n",
    "    \n",
    "    return {\n",
    "        'cpu_percent': cpu_percent,\n",
    "        'ram_percent': ram_percent,\n",
    "        'gpu_util': gpu_util,\n",
    "        'gpu_memory': gpu_memory\n",
    "    }\n",
    "\n",
    "def prepare_data(df, tokenizer, config):\n",
    "    df['Target'] = df['Target'].map({0: 0, 4: 1})\n",
    "    \n",
    "    # Faster sampling\n",
    "    n_samples = len(df) // (2 * config['portion'])\n",
    "    df_sampled = pd.concat([\n",
    "        df[df['Target'] == 0].sample(n=n_samples, random_state=42),\n",
    "        df[df['Target'] == 1].sample(n=n_samples, random_state=42)\n",
    "    ]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df_sampled, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Create datasets with minimal overhead\n",
    "    train_dataset = TweetDataset(\n",
    "        texts=train_df['Text'].values,\n",
    "        targets=train_df['Target'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=config['max_len']\n",
    "    )\n",
    "    \n",
    "    val_dataset = TweetDataset(\n",
    "        texts=val_df['Text'].values,\n",
    "        targets=val_df['Target'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=config['max_len']\n",
    "    )\n",
    "    \n",
    "    # Efficient DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'] * 2,  # Larger batches for validation\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, accuracy, config, checkpoint_dir):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}_{timestamp}.pt')\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'config': config\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, device, checkpoint_dir):\n",
    "    metrics = MetricsTracker()\n",
    "    metrics.start_training()\n",
    "    \n",
    "    # Get initial model size\n",
    "    model_size_mb = get_model_size(model)\n",
    "    print(f\"\\nModel Size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=len(train_loader) // 10,\n",
    "        num_training_steps=len(train_loader) * config['epochs']\n",
    "    )\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        metrics.start_epoch()\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc='Training')):\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Move data to GPU\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            # Forward and backward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets\n",
    "            )\n",
    "            \n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Record metrics\n",
    "            batch_time = time.time() - batch_start\n",
    "            metrics.add_batch_training_time(batch_time, len(input_ids))\n",
    "            running_loss += outputs.loss.item()\n",
    "            \n",
    "            # Monitor resources every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                resources = get_system_resources()\n",
    "                metrics.add_resource_usage(\n",
    "                    resources['cpu_percent'],\n",
    "                    resources['ram_percent'],\n",
    "                    resources['gpu_util'],\n",
    "                    resources['gpu_memory']\n",
    "                )\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                batch_start = time.time()\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=targets\n",
    "                )\n",
    "                \n",
    "                batch_time = time.time() - batch_start\n",
    "                metrics.add_batch_evaluation_time(batch_time, len(input_ids))\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        metrics.add_accuracy(epoch_accuracy)\n",
    "        metrics.end_epoch()\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "        print(f\"Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Validation Accuracy: {epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        # Save if best model\n",
    "        if epoch_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_accuracy\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch + 1,\n",
    "                          val_loss/len(val_loader), epoch_accuracy,\n",
    "                          config, checkpoint_dir)\n",
    "    \n",
    "    # Calculate and print final metrics\n",
    "    final_metrics = metrics.get_metrics()\n",
    "    print(\"\\nFinal Training Metrics:\")\n",
    "    print(f\"Average Training Time per Sample: {final_metrics['avg_training_time_per_sample']*1000:.2f} ms\")\n",
    "    print(f\"Average Evaluation Time per Sample: {final_metrics['avg_evaluation_time_per_sample']*1000:.2f} ms\")\n",
    "    print(f\"First Epoch Accuracy: {final_metrics['first_epoch_accuracy']:.2f}%\")\n",
    "    print(f\"Final Accuracy: {final_metrics['final_accuracy']:.2f}%\")\n",
    "    print(f\"Average Epoch Time: {final_metrics['avg_epoch_time']:.2f} seconds\")\n",
    "    print(f\"Total Training Time: {final_metrics['total_training_time']:.2f} seconds\")\n",
    "    print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "    print(\"\\nAverage Resource Usage:\")\n",
    "    print(f\"CPU Usage: {final_metrics['avg_resources']['cpu']:.1f}%\")\n",
    "    print(f\"RAM Usage: {final_metrics['avg_resources']['ram']:.1f}%\")\n",
    "    print(f\"GPU Utilization: {final_metrics['avg_resources']['gpu_util']:.1f}%\")\n",
    "    print(f\"GPU Memory Usage: {final_metrics['avg_resources']['gpu_memory']:.1f}%\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_path = os.path.join(checkpoint_dir, 'training_metrics.txt')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(\"Training Metrics:\\n\")\n",
    "        for key, value in final_metrics.items():\n",
    "            if key == 'avg_resources':\n",
    "                f.write(\"\\nAverage Resource Usage:\\n\")\n",
    "                for resource, usage in value.items():\n",
    "                    f.write(f\"{resource}: {usage:.1f}%\\n\")\n",
    "            else:\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        f.write(f\"\\nModel Size: {model_size_mb:.2f} MB\\n\")\n",
    "    \n",
    "    return final_metrics\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'max_len': 48,\n",
    "        'batch_size': 16, \n",
    "        'portion': 1,\n",
    "        'epochs': 3,\n",
    "        'learning_rate': 2e-5\n",
    "    }\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = 'model_checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(\n",
    "        f\"{path}/tweets.csv\",\n",
    "        encoding=\"ISO-8859-1\",\n",
    "        names=[\"Target\", \"ID\", \"Date\", \"Query\", \"User\", \"Text\"],\n",
    "        header=None\n",
    "    )\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    print(\"\\nInitializing model and tokenizer...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    train_loader, val_loader = prepare_data(df, tokenizer, CONFIG)\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_model(model, train_loader, val_loader, CONFIG, device, checkpoint_dir)\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
