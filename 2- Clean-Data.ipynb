{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "UDMFfSJK9pSY",
        "outputId": "c2597f64-2047-4472-d1bc-77ba4b263d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tokenization and stopword removal complete!\n",
            "Cleaned data saved to: ./1st_cleaned_data.csv\n",
            "                                                Text  \\\n",
            "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
            "1  is upset that he can't update his Facebook by ...   \n",
            "2  @Kenichan I dived many times for the ball. Man...   \n",
            "3    my whole body feels itchy and like its on fire    \n",
            "4  @nationwideclass no, it's not behaving at all....   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  a that s a bummer you shoulda got david carr o...   \n",
            "1  is upset that he can t update his facebook by ...   \n",
            "2  i dived many times for the ball managed to sav...   \n",
            "3     my whole body feels itchy and like its on fire   \n",
            "4  no it s not behaving at all i m mad why am i h...   \n",
            "\n",
            "                                       cleaned_words  \\\n",
            "0    [bummer, shoulda, got, david, carr, third, day]   \n",
            "1  [upset, update, facebook, texting, might, cry,...   \n",
            "2  [dived, many, times, ball, managed, save, rest...   \n",
            "3            [whole, body, feels, itchy, like, fire]   \n",
            "4                               [behaving, mad, see]   \n",
            "\n",
            "                                       cleaned_grams  \n",
            "0  [bummer shoulda, shoulda got, got david, david...  \n",
            "1  [upset update, update facebook, facebook texti...  \n",
            "2  [dived many, many times, times ball, ball mana...  \n",
            "3  [whole body, body feels, feels itchy, itchy li...  \n",
            "4                            [behaving mad, mad see]  \n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Download necessary NLTK resources (only needed once)\n",
        "# Uncomment these lines if running for the first time\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# Step 2: Load the Dataset\n",
        "file_path = \"./processed_data.csv\"  # Change this if needed\n",
        "df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "# Step 3: Define Stopwords and Precompile Regex Patterns\n",
        "stop_words = set(stopwords.words('english'))\n",
        "url_pattern = re.compile(r'http\\S+|www\\S+')\n",
        "mention_pattern = re.compile(r'@\\w+')\n",
        "hashtag_pattern = re.compile(r'#\\w+')\n",
        "non_alpha_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "\n",
        "# Step 4: Clean 'Text' Column\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "        # Remove URLs, mentions, hashtags\n",
        "        text = url_pattern.sub('', text)\n",
        "        text = mention_pattern.sub('', text)\n",
        "        text = hashtag_pattern.sub('', text)\n",
        "        # Remove non-alphabetic characters\n",
        "        text = non_alpha_pattern.sub(' ', text)\n",
        "        # Replace multiple spaces with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Strip leading and trailing spaces\n",
        "        text = text.strip()\n",
        "        return text\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# Apply the cleaning function to 'Text' column\n",
        "df['cleaned_text'] = df['Text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Step 5: Tokenize and Remove Stopwords\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# Apply function to get 'cleaned_words' column\n",
        "df['cleaned_words'] = df['cleaned_text'].apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "# Step 6: Generate N-grams from 'cleaned_words' Column\n",
        "def generate_ngrams(words_list, n=2):\n",
        "    # Generate n-grams only if there are enough words\n",
        "    if len(words_list) >= n:\n",
        "        return [' '.join(gram) for gram in ngrams(words_list, n)]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Apply function to get 'cleaned_grams' column\n",
        "df['cleaned_grams'] = df['cleaned_words'].apply(generate_ngrams)\n",
        "\n",
        "# Step 7: Save Cleaned Data to a New CSV File\n",
        "output_path = \"./cleaned_data.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "# Step 8: Display Success Message and First Few Rows\n",
        "print(\"✅ Tokenization and stopword removal complete!\")\n",
        "print(\"Cleaned data saved to:\", output_path)\n",
        "print(df[['Text', 'cleaned_text', 'cleaned_words', 'cleaned_grams']].head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
