{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPI Twitter Sentiment Analysis Project\n",
    "\n",
    "## Data Analysis (Tokens frequency for each sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>words</th>\n",
       "      <th>grams</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>cleaned_grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>['switchfoot', 'http', '', 'twitpiccom2y1zl', ...</td>\n",
       "      <td>['switchfoot http', 'http ', ' twitpiccom2y1zl...</td>\n",
       "      <td>['switchfoot']</td>\n",
       "      <td>[]</td>\n",
       "      <td>a that s a bummer you shoulda got david carr o...</td>\n",
       "      <td>['bummer', 'shoulda', 'got', 'david', 'carr', ...</td>\n",
       "      <td>['bummer shoulda', 'shoulda got', 'got david',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>['is', 'upset', 'that', 'he', 'ca', 'nt', 'upd...</td>\n",
       "      <td>['is upset', 'upset that', 'that he', 'he ca',...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>is upset that he can t update his facebook by ...</td>\n",
       "      <td>['upset', 'update', 'facebook', 'texting', 'mi...</td>\n",
       "      <td>['upset update', 'update facebook', 'facebook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>['Kenichan', 'I', 'dived', 'many', 'times', 'f...</td>\n",
       "      <td>['Kenichan I', 'I dived', 'dived many', 'many ...</td>\n",
       "      <td>['Kenichan']</td>\n",
       "      <td>[]</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>['dived', 'many', 'times', 'ball', 'managed', ...</td>\n",
       "      <td>['dived many', 'many times', 'times ball', 'ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>['my', 'whole', 'body', 'feels', 'itchy', 'and...</td>\n",
       "      <td>['my whole', 'whole body', 'body feels', 'feel...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>['whole', 'body', 'feels', 'itchy', 'like', 'f...</td>\n",
       "      <td>['whole body', 'body feels', 'feels itchy', 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>['nationwideclass', 'no', '', 'it', 's', 'not'...</td>\n",
       "      <td>['nationwideclass no', 'no ', ' it', 'it s', '...</td>\n",
       "      <td>['nationwideclass']</td>\n",
       "      <td>[]</td>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "      <td>['behaving', 'mad', 'see']</td>\n",
       "      <td>['behaving mad', 'mad see']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target          ID                          Date     Query  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              User                                               Text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                               words  \\\n",
       "0  ['switchfoot', 'http', '', 'twitpiccom2y1zl', ...   \n",
       "1  ['is', 'upset', 'that', 'he', 'ca', 'nt', 'upd...   \n",
       "2  ['Kenichan', 'I', 'dived', 'many', 'times', 'f...   \n",
       "3  ['my', 'whole', 'body', 'feels', 'itchy', 'and...   \n",
       "4  ['nationwideclass', 'no', '', 'it', 's', 'not'...   \n",
       "\n",
       "                                               grams             mentions  \\\n",
       "0  ['switchfoot http', 'http ', ' twitpiccom2y1zl...       ['switchfoot']   \n",
       "1  ['is upset', 'upset that', 'that he', 'he ca',...                   []   \n",
       "2  ['Kenichan I', 'I dived', 'dived many', 'many ...         ['Kenichan']   \n",
       "3  ['my whole', 'whole body', 'body feels', 'feel...                   []   \n",
       "4  ['nationwideclass no', 'no ', ' it', 'it s', '...  ['nationwideclass']   \n",
       "\n",
       "  hashtags                                       cleaned_text  \\\n",
       "0       []  a that s a bummer you shoulda got david carr o...   \n",
       "1       []  is upset that he can t update his facebook by ...   \n",
       "2       []  i dived many times for the ball managed to sav...   \n",
       "3       []     my whole body feels itchy and like its on fire   \n",
       "4       []  no it s not behaving at all i m mad why am i h...   \n",
       "\n",
       "                                       cleaned_words  \\\n",
       "0  ['bummer', 'shoulda', 'got', 'david', 'carr', ...   \n",
       "1  ['upset', 'update', 'facebook', 'texting', 'mi...   \n",
       "2  ['dived', 'many', 'times', 'ball', 'managed', ...   \n",
       "3  ['whole', 'body', 'feels', 'itchy', 'like', 'f...   \n",
       "4                         ['behaving', 'mad', 'see']   \n",
       "\n",
       "                                       cleaned_grams  \n",
       "0  ['bummer shoulda', 'shoulda got', 'got david',...  \n",
       "1  ['upset update', 'update facebook', 'facebook ...  \n",
       "2  ['dived many', 'many times', 'times ball', 'ba...  \n",
       "3  ['whole body', 'body feels', 'feels itchy', 'i...  \n",
       "4                        ['behaving mad', 'mad see']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target    Token  Type\n",
      "0       0   bummer  Word\n",
      "0       0  shoulda  Word\n",
      "0       0      got  Word\n",
      "0       0    david  Word\n",
      "0       0     carr  Word\n",
      "Target   Token  Type  Overall Frequency  Freq Neg  Freq Pos\n",
      "0           aa  Word                240       156        84\n",
      "177        aaa  Word                157        97        60\n",
      "289       aaaa  Word                 81        47        34\n",
      "345      aaaaa  Word                 39        27        12\n",
      "381     aaaaaa  Word                 30        17        13\n",
      "✅ Token frequencies saved to: ./token_frequencies.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('./cleaned_data.csv', low_memory=False)\n",
    "\n",
    "# Function to convert string representations of lists back to lists\n",
    "def convert_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Convert 'cleaned_words' and 'cleaned_grams' columns from strings to lists\n",
    "df['cleaned_words'] = df['cleaned_words'].apply(convert_to_list)\n",
    "df['cleaned_grams'] = df['cleaned_grams'].apply(convert_to_list)\n",
    "\n",
    "# Now proceed with processing\n",
    "# Process 'cleaned_words' (Words)\n",
    "words_df = df[['Target', 'cleaned_words']].explode('cleaned_words')\n",
    "words_df = words_df.rename(columns={'cleaned_words': 'Token'})\n",
    "words_df['Type'] = 'Word'\n",
    "\n",
    "print(words_df.head())\n",
    "\n",
    "# Process 'cleaned_grams' (Bi-Grams)\n",
    "bigrams_df = df[['Target', 'cleaned_grams']].explode('cleaned_grams')\n",
    "bigrams_df = bigrams_df.rename(columns={'cleaned_grams': 'Token'})\n",
    "bigrams_df['Type'] = 'Bi-Gram'\n",
    "\n",
    "# Combine 'words_df' and 'bigrams_df' into a single DataFrame\n",
    "tokens_df = pd.concat([words_df, bigrams_df], ignore_index=True)\n",
    "\n",
    "# Ensure 'Token' column has string type for consistency\n",
    "tokens_df['Token'] = tokens_df['Token'].astype(str)\n",
    "\n",
    "# Proceed with the rest of the processing...\n",
    "\n",
    "# Step 2: Group by 'Token', 'Type', and 'Target' to calculate frequencies\n",
    "\n",
    "# Calculate frequency counts\n",
    "freq = (\n",
    "    tokens_df.groupby(['Token', 'Type', 'Target'])\n",
    "    .size()\n",
    "    .reset_index(name='Frequency')\n",
    ")\n",
    "\n",
    "# Step 3: Pivot the table to get frequencies for each sentiment\n",
    "\n",
    "# Pivot the DataFrame to have separate columns for negative and positive frequencies\n",
    "freq_pivot = freq.pivot_table(\n",
    "    index=['Token', 'Type'],\n",
    "    columns='Target',\n",
    "    values='Frequency',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "freq_pivot = freq_pivot.rename(columns={\n",
    "    0: 'Freq Neg',\n",
    "    4: 'Freq Pos'\n",
    "})\n",
    "\n",
    "# Ensure 'Freq Neg' and 'Freq Pos' columns exist even if there are no occurrences\n",
    "if 'Freq Neg' not in freq_pivot.columns:\n",
    "    freq_pivot['Freq Neg'] = 0\n",
    "if 'Freq Pos' not in freq_pivot.columns:\n",
    "    freq_pivot['Freq Pos'] = 0\n",
    "\n",
    "# Step 4: Calculate the Overall Frequency\n",
    "\n",
    "freq_pivot['Overall Frequency'] = freq_pivot['Freq Neg'] + freq_pivot['Freq Pos']\n",
    "\n",
    "# Reorder the columns\n",
    "final_df = freq_pivot[['Token', 'Type', 'Overall Frequency', 'Freq Neg', 'Freq Pos']]\n",
    "\n",
    "# Remove Tokens with 1 overall frequency\n",
    "final_df = final_df[final_df['Overall Frequency'] > 10]\n",
    "\n",
    "# Remove 'nan' Bi-Gram Token\n",
    "final_df = final_df[final_df['Token'] != 'nan']\n",
    "\n",
    "# Step 5: Display or Save the Result\n",
    "\n",
    "print(final_df.head())\n",
    "\n",
    "# Optionally, save the final DataFrame to a CSV file\n",
    "output_path = \"./token_frequencies.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(\"✅ Token frequencies saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Target</th>\n",
       "      <th>Token</th>\n",
       "      <th>Type</th>\n",
       "      <th>Overall Frequency</th>\n",
       "      <th>Freq Neg</th>\n",
       "      <th>Freq Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980707</th>\n",
       "      <td>last night</td>\n",
       "      <td>Bi-Gram</td>\n",
       "      <td>12524</td>\n",
       "      <td>7556</td>\n",
       "      <td>4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461242</th>\n",
       "      <td>good morning</td>\n",
       "      <td>Bi-Gram</td>\n",
       "      <td>10168</td>\n",
       "      <td>1943</td>\n",
       "      <td>8225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085130</th>\n",
       "      <td>wish could</td>\n",
       "      <td>Bi-Gram</td>\n",
       "      <td>6816</td>\n",
       "      <td>5782</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192504</th>\n",
       "      <td>feel like</td>\n",
       "      <td>Bi-Gram</td>\n",
       "      <td>6559</td>\n",
       "      <td>5129</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117269</th>\n",
       "      <td>looking forward</td>\n",
       "      <td>Bi-Gram</td>\n",
       "      <td>5624</td>\n",
       "      <td>2195</td>\n",
       "      <td>3429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Target             Token     Type  Overall Frequency  Freq Neg  Freq Pos\n",
       "1980707       last night  Bi-Gram              12524      7556      4968\n",
       "1461242     good morning  Bi-Gram              10168      1943      8225\n",
       "4085130       wish could  Bi-Gram               6816      5782      1034\n",
       "1192504        feel like  Bi-Gram               6559      5129      1430\n",
       "2117269  looking forward  Bi-Gram               5624      2195      3429"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df['Type']=='Bi-Gram'].sort_values(by='Overall Frequency', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Target</th>\n",
       "      <th>Token</th>\n",
       "      <th>Type</th>\n",
       "      <th>Overall Frequency</th>\n",
       "      <th>Freq Neg</th>\n",
       "      <th>Freq Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1457262</th>\n",
       "      <td>good</td>\n",
       "      <td>Word</td>\n",
       "      <td>91324</td>\n",
       "      <td>29207</td>\n",
       "      <td>62117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840831</th>\n",
       "      <td>day</td>\n",
       "      <td>Word</td>\n",
       "      <td>89556</td>\n",
       "      <td>41373</td>\n",
       "      <td>48183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378972</th>\n",
       "      <td>get</td>\n",
       "      <td>Word</td>\n",
       "      <td>82172</td>\n",
       "      <td>45610</td>\n",
       "      <td>36562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041425</th>\n",
       "      <td>like</td>\n",
       "      <td>Word</td>\n",
       "      <td>78573</td>\n",
       "      <td>41053</td>\n",
       "      <td>37520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423636</th>\n",
       "      <td>go</td>\n",
       "      <td>Word</td>\n",
       "      <td>74006</td>\n",
       "      <td>45634</td>\n",
       "      <td>28372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Target  Token  Type  Overall Frequency  Freq Neg  Freq Pos\n",
       "1457262  good  Word              91324     29207     62117\n",
       "840831    day  Word              89556     41373     48183\n",
       "1378972   get  Word              82172     45610     36562\n",
       "2041425  like  Word              78573     41053     37520\n",
       "1423636    go  Word              74006     45634     28372"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df['Type']=='Word'].sort_values(by='Overall Frequency', ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
